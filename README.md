## Predicting Home Prices using data from 3 US states [MA, PA, RI]
### Jose L Garcia
### April, 2019

* Buying a house represents one of the most important decisions for most people. On average, a mortgage term is 30 years long, which makes it easy to see the huge impact of such decision in someones' life, in economic terms and the implications of moving to a place where one could feel safe, kids could have access to schools, recreation and commercial areas, etc. On the other hand, buying a house can be seen as an investment. Maximizing the value of that investment is critical. Paying the right amount for the right house should be in the mind of anyone trying to find a new house.

* Just recently, in January 2019, online real state marketplace Zillow awarded a million dollar prize to the winning team that predicted house sale prices from three California counties using data from two years 2016-2017. The winning team worked on the problem for more than a year to get a predictive score of 0.1211. Zillow's benchmark score was set to 0.13. The predictive model beat the benchmark score after correctly predicting the actual value of home sales over a three month evaluation period. Zillow's competition is a reflection of the importance of solving a problem like this one, where we are trying to maximize the return of investment of new home buyers, while getting the house they need at the right price based on dozen of different variables.

* In Zillowâ€™s case, the results were accomplished with a two year dataset. For our project, we will be working with a dataset containing 13 years of home sales from three northeast US states: Massachusetts, Pennsylvania and Rhode Island. This huge amount of transactions will be helpful to create an accurate predictive model that could be generalized to the remaining US states. Additionally, we will be integrating demographic data from the US Census Bureau, which we believe can improve the accuracy of the predictions.

* Our results will be presented as a simple web app containing a map that will show the home prices over time. It will be stored in an EC2 instance on AWS.

* The main data source was provided by Citizens Bank. It contains 4.4 million rows x 73 columns. Rows represent home sale transactions over the last 13 years for MA, PA and RI. Each column represents a feature of the house and the transaction, such as number of bedrooms, building square feet, type of AC, roof shape, market valuation, actual sale price, sale date, etc.
The second data source contains demographic and average income information of the population in each US state. It is an open data source provided by the US Census Bureau. The idea is to link the two datasets and cross reference demographic information to enrich the main dataset and possibly get better predictions when taking into account demographics and income information of the different zip codes.

* One possible problem with this dataset is that it was provided in a CSV format that's over 2Gb. Even when that doesn't qualify as big data, the amount of time it takes to load to memory is considerable. When monitoring the available memory while doing simple operations with the data we realized it was not realistic to work with this dataset on a desktop computer. We already set up an EC2 instance with enough RAM and computing power to continue the Exploratory Data Analysis.

* Another problem with the data is the missing and/or inaccurate values. During the first data exploration we found out around 750,000 missing values for latitude-longitude which are important features for presenting the data on a map. On top of that, when doing the first heat map we realized some data points are incorrect because we see datapoints all over the US, as opposed to the information of the "Property State" column where we only see "MA", "PA" and "RI". To mitigate this issue, we will be using the Google Maps API. We are currently working on building an automated process that will read the address on each incorrect or missing row and get the correct [latitude, longitude] values.

* Once the missing values are filled the most important thing we need to work on will be figuring out what are the most important features, and compare different predictive models so we can at the end select the most accurate one.

* In terms of evaluating the model, since the dataset was provided by Citizens Bank, they set the evaluation metric too. Prediction accuracy will be determined by the highest proportion of predicted home sales that are within 10% of the actual home sale value.
